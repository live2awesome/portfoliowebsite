[{"content":"Abstract: In India, approximately 70% of the population depends on agriculture. However, crop productivity has not kept up with the increasing demand, necessitating the use of advanced technology. This project focuses on smart farming and precision farming, utilizing IoT, sensors, controllers, and AI/ML to predict suitable crops based on soil and environmental parameters.\nB. Objectives of the Project Study different soil types and crops suitable for each type. Identify and select sensors for soil and environment parameters. Create classification and prediction models for soil and crops. Train, test, and deploy the system on a suitable cloud platform. Build an Android app for farmers to interact with the system. Project Architecture: C. Scope \u0026amp; Limitations Scope:\nDecision-making support for farmers. Soil health awareness. Economic benefits in the agriculture sector. Limitations:\nDynamic nature of soil. Changing weather conditions. Uncertainty in environmental factors. GitHub Repository: IOT-based-Soil-Classification-and-Crop-Prediction\n2.1 Technological Used AWS IoT Analytics Amplify Framework (Serverless Application) Arduino IDE GitHub Git AWS IoT Core Amazon SageMaker AWS API Gateway Amazon Elastic Compute Cloud (EC2) Jupyter Notebook Android Studio NodeMCU. 8. Performance Analysis The project\u0026rsquo;s performance is based on the classification and prediction model accuracy. Integration of IoT devices with Android Application and AWS ensures accurate sensor readings. Soil classification and crop prediction models provide accuracy between 65% to 85%. Predicted crop list is displayed on the Android application. 9. Application Increase crop production yield rate. Maximize profit for farmers. Ensure crop availability for a healthy society. Reduce uncertainty in farming. Analyze resources and infrastructure for a specific time span. Reduce excessive chemical usage with precision agriculture. 10. Conclusion The developed system successfully predicts crops based on soil and environmental parameters. Android application integration with IoT sensors and Machine Learning deployed models was achieved. 11. Screenshots Login Page OTP Received Connect to IOT Kit Get Longitude and Latitude Soil Classification Page Crop Prediction Results through Postman 12 Literature Review F. Tseng et al. [4]: Big data analysis of farms based on Intelligent Agriculture. S. Liu et al. [5]: IoT and cloud-based monitoring systems for agriculture management. R. Priya et al. [6]: Efficient crop recommendation system. ","permalink":"https://live2awesome.github.io/portfoliowebsite/projects/iotbasedsoilclassificationandcropprediction/","summary":"Abstract: In India, approximately 70% of the population depends on agriculture. However, crop productivity has not kept up with the increasing demand, necessitating the use of advanced technology. This project focuses on smart farming and precision farming, utilizing IoT, sensors, controllers, and AI/ML to predict suitable crops based on soil and environmental parameters.\nB. Objectives of the Project Study different soil types and crops suitable for each type. Identify and select sensors for soil and environment parameters.","title":"IOT based Soil Classification and Crop Prediction"},{"content":"Project Overview The \u0026ldquo;Online Course Coordination System for Employees\u0026rdquo; is designed to efficiently schedule training sessions for candidates within a company. This system is implemented using the C programming language, and it operates without the need for a traditional database. Instead, file handling operations with .txt files are employed for seamless data storage and retrieval.\nKey Features Training Session Scheduling:\nThe system enables the scheduling of training sessions for employees, allowing for efficient organization and coordination. File Handling Operations:\nData persistence is achieved through file handling operations. The system utilizes .txt files to store and retrieve relevant information, eliminating the need for a database. User-Friendly Interface:\nThe user interface is designed to be intuitive, facilitating easy interaction for scheduling, viewing, and managing training sessions. Data Integrity and Security:\nDespite not using a database, the system ensures data integrity and security through careful file handling practices, such as error checking and validation. Functionality Schedule Training Sessions:\nUsers can input and schedule training sessions for employees, specifying details such as date, time, and location. Data Storage in .txt Files:\nAll relevant information regarding training sessions is stored in .txt files, ensuring persistent data storage. Retrieve Training Session Information:\nUsers can retrieve and view information about scheduled training sessions, allowing for easy monitoring and coordination. Error Handling:\nThe system incorporates robust error handling mechanisms to address issues and ensure the integrity of stored data. Workflow Input Training Session Details:\nUsers input details for scheduling training sessions, including date, time, and location. Save to .txt File:\nThe entered data is saved to a .txt file, ensuring persistent storage. Retrieve Information:\nUsers can retrieve and view information about scheduled training sessions from the .txt file. Error Checking:\nThe system includes error-checking mechanisms to handle invalid inputs or unforeseen issues during data retrieval. Benefits and Applications Simplified Coordination:\nThe system simplifies the coordination of training sessions, providing an organized approach to scheduling. Resource Efficiency:\nBy utilizing file handling operations, the project achieves resource efficiency without the need for a complex database setup. Scalability:\nThe system can be easily scaled to accommodate additional features or enhancements in the future. This project showcases the effective use of file handling operations in C programming to create a streamlined Online Course Coordination System for Employees, ensuring simplicity, efficiency, and ease of use.\n","permalink":"https://live2awesome.github.io/portfoliowebsite/projects/onlinecourseco-ordinationsystem/","summary":"Project Overview The \u0026ldquo;Online Course Coordination System for Employees\u0026rdquo; is designed to efficiently schedule training sessions for candidates within a company. This system is implemented using the C programming language, and it operates without the need for a traditional database. Instead, file handling operations with .txt files are employed for seamless data storage and retrieval.\nKey Features Training Session Scheduling:\nThe system enables the scheduling of training sessions for employees, allowing for efficient organization and coordination.","title":"Online Course Co-ordination System"},{"content":"Project Description The \u0026ldquo;YouTube Summarizer with GPT\u0026rdquo; project is a personal initiative aimed at exploring the capabilities of the OpenAI GPT (Generative Pre-trained Transformer) model for text summarization in the context of Natural Language Processing (NLP). The goal was to gain a deep understanding of Large Language Models (LLM) and harness the power of the OpenAI GPT API for creating a YouTube video summarization tool.\nKey Features and Learning Objectives Understanding LLM and Text Summarization:\nCreated a Jupyter Notebook to delve into the concepts of Large Language Models and the fundamentals of text summarization. Explored how OpenAI\u0026rsquo;s GPT API can be utilized for extracting meaningful summaries from textual content. Integration with YouTube Data:\nExplored techniques to fetch and process data from YouTube videos, preparing it for input to the summarization model. Investigated methods to handle video transcripts and convert them into a format suitable for GPT-based summarization. GPT API Integration:\nUtilized the OpenAI GPT API to generate concise and coherent summaries for YouTube videos. Explored the capabilities and limitations of the GPT model in the context of video content summarization. Technical Highlights Technology Stack: Python, Jupyter Notebook, OpenAI GPT API Codebase: \u0026ndash; Jupyter Notebook: YouTube Summarizer with GPT Outcome: Gained hands-on experience in working with OpenAI\u0026rsquo;s GPT API. Improved understanding of text summarization techniques and their application to real-world problems. Future Enhancements This project serves as a foundation for future enhancements and features. Potential areas for improvement include:\nEnhanced User Interface: Developing a user-friendly interface for users to interact with the YouTube Summarizer tool. Scalability: Exploring ways to scale the summarization process for a larger volume of videos. Multi-language Support: Extending the tool to support summarization for videos in multiple languages. Acknowledgments Special thanks to OpenAI for providing access to the GPT API and contributing to advancements in natural language understanding.\n","permalink":"https://live2awesome.github.io/portfoliowebsite/projects/youtubesummarizerwithgpt/","summary":"Project Description The \u0026ldquo;YouTube Summarizer with GPT\u0026rdquo; project is a personal initiative aimed at exploring the capabilities of the OpenAI GPT (Generative Pre-trained Transformer) model for text summarization in the context of Natural Language Processing (NLP). The goal was to gain a deep understanding of Large Language Models (LLM) and harness the power of the OpenAI GPT API for creating a YouTube video summarization tool.\nKey Features and Learning Objectives Understanding LLM and Text Summarization:","title":"Youtube Summarizer with GPT"},{"content":"Project Overview This project focuses on analyzing user activity by tracking their visits to various websites within a browser. The goal is to predict user patterns through the analysis of web cookies stored in the browser. To gather necessary data parameters, a JavaScript script is executed on the RapidMiner server, and the subsequent user behavior prediction is processed using RapidMiner Studio.\nKey Components and Processes Data Collection:\nThe project involves collecting user data through web cookies, which store information about a user\u0026rsquo;s interactions with different websites. JavaScript Script:\nA custom JavaScript script is employed to extract relevant data parameters from the web cookies. This script is executed on the RapidMiner server to ensure efficient data retrieval. RapidMiner Studio:\nRapidMiner Studio is utilized as the primary tool for processing and analyzing the collected data. It provides a comprehensive environment for data preparation, machine learning, and predictive modeling. User Behavior Prediction:\nThe core objective is to predict user behavior patterns based on the analyzed data. Machine learning techniques and predictive modeling algorithms are applied within RapidMiner Studio for accurate predictions. Project Workflow Data Collection:\nWeb cookies are collected from users\u0026rsquo; browsers during their interactions with various websites. JavaScript Execution:\nThe JavaScript script is executed on the RapidMiner server to extract pertinent data parameters from the collected web cookies. Data Processing in RapidMiner Studio:\nThe extracted data is imported into RapidMiner Studio for further analysis and preprocessing. Machine Learning and Prediction:\nMachine learning algorithms within RapidMiner Studio are employed to build models that predict user behavior based on historical data. Evaluation and Iteration:\nThe predictive models are evaluated for accuracy, and the project undergoes iterative refinement to enhance prediction capabilities. Benefits and Applications Personalized User Experience:\nPredicting user behavior enables the creation of personalized user experiences on websites. Marketing Optimization:\nBusinesses can optimize marketing strategies based on predicted user preferences and activities. Enhanced User Engagement:\nTailoring content and recommendations based on predicted behavior enhances overall user engagement. This project showcases the application of web mining techniques to gain valuable insights into user behavior, offering practical implications for businesses and website optimization.\n","permalink":"https://live2awesome.github.io/portfoliowebsite/projects/userbehaviourpredictionthroughwebmining/","summary":"Project Overview This project focuses on analyzing user activity by tracking their visits to various websites within a browser. The goal is to predict user patterns through the analysis of web cookies stored in the browser. To gather necessary data parameters, a JavaScript script is executed on the RapidMiner server, and the subsequent user behavior prediction is processed using RapidMiner Studio.\nKey Components and Processes Data Collection:\nThe project involves collecting user data through web cookies, which store information about a user\u0026rsquo;s interactions with different websites.","title":"User Behaviour Prediction through Session Web Mining"},{"content":"Project Overview The \u0026ldquo;Blockchain-based Crowdfunding Platform\u0026rdquo; is a decentralized alternative to websites like Kickstarter.com, introducing transparency and security through the implementation of blockchain technology. The project leverages blockchain-based smart contracts using the Ethereum framework to ensure transparent and secure transactions within the crowdfunding platform.\nKey Features Decentralized Architecture:\nThe platform operates on a decentralized network, eliminating the need for a central authority and providing enhanced security. Smart Contracts:\nBlockchain-based smart contracts, implemented using the Ethereum framework, automate and enforce the rules of crowdfunding transactions, ensuring transparency and trust. Transparent Transactions:\nThe use of blockchain ensures transparency in all transactions, allowing contributors to verify and trace the flow of funds throughout the crowdfunding process. Enhanced Security:\nBlockchain\u0026rsquo;s cryptographic principles provide a secure and tamper-resistant environment, reducing the risk of fraud or unauthorized access. Tokenization:\nThe platform may implement tokenization, allowing contributors to receive project-specific tokens representing their investments. Functionality Project Creation:\nUsers can create crowdfunding projects, specifying details such as funding goals, project descriptions, and timelines. Contribution through Cryptocurrency:\nContributors can fund projects using cryptocurrencies such as Ether (ETH), with transactions recorded on the blockchain. Smart Contract Execution:\nSmart contracts automatically execute crowdfunding rules, releasing funds to project creators when funding goals are met. Transparent Project Tracking:\nContributors can track the progress of funded projects transparently on the blockchain. Decentralized Governance:\nThe platform may implement decentralized governance models, allowing token holders to participate in decision-making processes. Workflow Project Creation:\nProject creators define their crowdfunding campaigns, setting funding goals, project details, and other relevant information. Contribution:\nContributors fund projects by sending cryptocurrencies (e.g., Ether) to the project\u0026rsquo;s smart contract address. Smart Contract Execution:\nSmart contracts automatically execute when funding goals are met, releasing funds to the project creators. Transparent Tracking:\nContributors can transparently track the flow of funds and the progress of funded projects on the blockchain. Decentralized Governance (Optional):\nGovernance mechanisms, if implemented, allow token holders to participate in decision-making related to the platform. Benefits and Applications Transparency:\nBlockchain ensures transparency, providing contributors with an immutable record of all transactions and project updates. Security:\nThe decentralized and cryptographic nature of blockchain enhances the security of transactions and project data. Trust:\nContributors can trust the crowdfunding platform due to the automated execution of rules through smart contracts. Global Accessibility:\nThe decentralized nature of the platform allows global participation, enabling projects to receive funding from contributors worldwide. Innovation in Crowdfunding:\nThe project represents an innovative and secure approach to crowdfunding, leveraging blockchain\u0026rsquo;s capabilities. The \u0026ldquo;Blockchain-based Crowdfunding Platform\u0026rdquo; transforms traditional crowdfunding by introducing decentralization, smart contracts, and transparency through blockchain technology, providing a secure and efficient way to fund and support innovative projects.\n","permalink":"https://live2awesome.github.io/portfoliowebsite/projects/blockchainbasedcrowdfundingplatform/","summary":"Project Overview The \u0026ldquo;Blockchain-based Crowdfunding Platform\u0026rdquo; is a decentralized alternative to websites like Kickstarter.com, introducing transparency and security through the implementation of blockchain technology. The project leverages blockchain-based smart contracts using the Ethereum framework to ensure transparent and secure transactions within the crowdfunding platform.\nKey Features Decentralized Architecture:\nThe platform operates on a decentralized network, eliminating the need for a central authority and providing enhanced security. Smart Contracts:\nBlockchain-based smart contracts, implemented using the Ethereum framework, automate and enforce the rules of crowdfunding transactions, ensuring transparency and trust.","title":"Blockchain based Crowd Funding Platform "},{"content":"Overview The \u0026ldquo;Blockchain-based Power Battery Replacement Platform\u0026rdquo; addresses the challenge of diverse vendors for electric vehicle batteries by providing a unified platform for Indian users. Leveraging blockchain technology, the platform treats batteries as assets, ensuring a seamless and secure replacement process. The associated Android app, developed using Flutter and Dart, integrates with the Ethereum network and MetaMask wallet, enabling efficient battery management.\nKey Features Unified Platform:\nOffers a centralized solution for electric vehicle battery replacements, streamlining the user experience. Blockchain Integration:\nEmploys blockchain logic to manage battery assets, ensuring transparency and traceability throughout the replacement process. Distributed Ledger:\nUtilizes Ethereum smart contracts written in Solidity to create a distributed ledger for transparent and secure transactions. Damage Handling:\nImplements a robust system for handling damaged batteries, involving manufacturers or scrap services based on the extent of damage. Android App (Flutter and Dart):\nThe associated Android app facilitates user interaction, offering a user-friendly interface for locating the nearest battery replacement shops. MetaMask Wallet Integration:\nConnects seamlessly with the MetaMask wallet through Dart packages, enhancing the security and accessibility of the app. Workflow User Request:\nUsers initiate a battery replacement request through the Flutter-based Android app. Blockchain Asset Management:\nBlockchain technology manages the battery as an asset, tracking its status and facilitating replacement. Smart Contract Execution:\nEthereum smart contracts, written in Solidity, govern the replacement process, ensuring a secure and automated transaction. Nearest Shop Locator:\nThe app uses intuitive features to locate the nearest battery replacement shops, enhancing user convenience. Transparent Tracking:\nBlockchain ensures transparency, allowing users to track the entire replacement process and battery status. Contributions Blockchain Integration:\nSuccessfully connected the Android app to the Ethereum network and MetaMask wallet, incorporating blockchain functionalities. Dart Package Enhancement:\nImproved the efficiency of Dart packages used in the app, contributing to a more responsive and streamlined user experience. Front-end Development:\nImplemented the front-end logic of the Android app using Flutter, focusing on user-centric design and functionality. Smart Contract Development:\nContributed to the development of Ethereum smart contracts in Solidity, ensuring secure and transparent execution of replacement transactions. Benefits User-Friendly Experience:\nProvides users with a simple and efficient platform for electric vehicle battery replacements. Blockchain Security:\nEnsures secure and transparent transactions through the use of blockchain technology. Asset Tracking:\nEnables transparent tracking of battery assets, from request initiation to replacement completion. Efficient Damage Handling:\nOptimizes the process of managing damaged batteries by directing them to manufacturers or scrap services based on the level of damage. The \u0026ldquo;Blockchain-based Power Battery Replacement Platform\u0026rdquo; stands as a pioneering solution, unifying battery replacement services for electric vehicles in India through blockchain-driven efficiency and a user-centric Android app.\n","permalink":"https://live2awesome.github.io/portfoliowebsite/projects/ba-treeblockchainpoweredbatteryreplacementplatform/","summary":"Overview The \u0026ldquo;Blockchain-based Power Battery Replacement Platform\u0026rdquo; addresses the challenge of diverse vendors for electric vehicle batteries by providing a unified platform for Indian users. Leveraging blockchain technology, the platform treats batteries as assets, ensuring a seamless and secure replacement process. The associated Android app, developed using Flutter and Dart, integrates with the Ethereum network and MetaMask wallet, enabling efficient battery management.\nKey Features Unified Platform:\nOffers a centralized solution for electric vehicle battery replacements, streamlining the user experience.","title":"Blockchain Powered Battery Replacement Platform(Ba-Tree)"},{"content":"Expert Python Pyspark Tensorflow Scikitlearn AWS Worked On Predictive Analytics Machine Learning Deep Learning Forecasting SQL Linux Docker Matplotlib REST API Pandas Numpy ApacheSpark Bitbucket Jira PowerBI Git HTML CSS Graphql Streamlit Keras node pytest R Postman Arduino jQuery Anaconda Flask Java C++ C Springboot Angular Redhat Plotly Seaborn Opencv Android Learn \u0026amp; Understand Azure Marklogic Kubernetes Kakfa Nosql Mongodb Solidity Metamask Truffle RapidMiner Want to Work On Dart Go Rust ","permalink":"https://live2awesome.github.io/portfoliowebsite/skills/","summary":"Expert Python Pyspark Tensorflow Scikitlearn AWS Worked On Predictive Analytics Machine Learning Deep Learning Forecasting SQL Linux Docker Matplotlib REST API Pandas Numpy ApacheSpark Bitbucket Jira PowerBI Git HTML CSS Graphql Streamlit Keras node pytest R Postman Arduino jQuery Anaconda Flask Java C++ C Springboot Angular Redhat Plotly Seaborn Opencv Android Learn \u0026amp; Understand Azure Marklogic Kubernetes Kakfa Nosql Mongodb Solidity Metamask Truffle RapidMiner Want to Work On Dart Go Rust ","title":"Skills Dashboard"},{"content":"Full-time Role | Itorizon Pvt Limited / UCBOS | Bangalore\nOverview As a Data Scientist at Itorizon Pvt Limited / UCBOS, I contributed significantly to various aspects of machine learning and data analysis. My role encompassed deploying neural network models, developing data analysis services, implementing cross-validation techniques, and exploring ensemble approaches for forecasting, regression, and classification modules.\nKey Achievements and Responsibilities Neural Network Deployment:\nDeployed classification and regression neural network models in the TensorFlow framework. Utilized batching techniques for efficient model training. Implemented auto-hyperparameter optimization, including epoch and dropout rate adjustments. Data Analysis Service:\nDeveloped robust data analysis services with effective visualization techniques. Applied normalization and transformation operations to enhance data quality. Conducted descriptive statistics for insightful data understanding. Cross-Validation Techniques:\nIntroduced cross-validation techniques for regression, classification, and forecasting modules in both TensorFlow and Pyspark frameworks. Ensemble Approach:\nImplemented ensemble approaches in the Pyspark framework for forecasting, regression, and classification modules. Explainable AI (XAI):\nWorked on SHAP (SHapley Additive exPlanations) plots in LSTM models for Explainable AI, enhancing machine learning interpretability. Forecasting Module Leadership:\nLed the forecasting module, demonstrating a high level of accountability and contributing to ML product development in the R\u0026amp;D department. Model Bias and Data Drift:\nWorked on developing and implementing modules focusing on Model Bias and Data Drift, highlighting a commitment to addressing critical issues in machine learning models. Learning and Growth Throughout this role, I gained valuable insights into machine learning product development, particularly in forecasting modules. The experience provided a deep understanding of neural network deployment, data analysis, and the importance of model explainability and fairness.\nThis role has significantly contributed to my expertise in the dynamic field of data science, showcasing a comprehensive skill set in both traditional machine learning frameworks and cutting-edge technologies.\n","permalink":"https://live2awesome.github.io/portfoliowebsite/experience/ucbos/","summary":"Full-time Role | Itorizon Pvt Limited / UCBOS | Bangalore\nOverview As a Data Scientist at Itorizon Pvt Limited / UCBOS, I contributed significantly to various aspects of machine learning and data analysis. My role encompassed deploying neural network models, developing data analysis services, implementing cross-validation techniques, and exploring ensemble approaches for forecasting, regression, and classification modules.\nKey Achievements and Responsibilities Neural Network Deployment:\nDeployed classification and regression neural network models in the TensorFlow framework.","title":"Data Scientist"},{"content":"Full-time Role | Tata Consultancy Services (TCS) | Nagpur,India\nOverview As an Assistant System Engineer at Tata Consultancy Services (TCS), I played a pivotal role in developing and enhancing a SAAS Cloud-Based Billing Solution with a multinational client, utilizing Zuora. My responsibilities extended to automating the data reporting workflow for financial data, optimizing services such as SAP, AWS S3 Bucket, and Revpro. Notable achievements include cost-saving initiatives and contributions to API development, SQL queries, and Liquid Programming for front-end services.\nKey Achievements and Responsibilities SAAS Cloud-Based Billing Solution:\nWorked closely on the development and enhancement of a SAAS Cloud-Based Billing Solution utilizing Zuora, catering to the requirements of a multinational client. Automated Data Reporting Workflow:\nAutomated the data reporting workflow for financial data, streamlining processes for upstream and downstream services like SAP, AWS S3 Bucket, and Revpro. Ensured seamless integration and efficient data flow between systems, enhancing overall operational efficiency. Cost Savings Initiatives:\nImplemented various email scheduling workflows and optimized invoicing templates, resulting in a cost-saving of $15,000 for email services. Demonstrated proficiency in creating efficient and cost-effective solutions, showcasing a keen understanding of financial implications in system design. API Development and SQL Queries:\nPlayed a key role in the creation of a robust REST API structure, ensuring effective communication between different services. Developed SQL queries for database operations, contributing to the efficiency and performance of backend systems. Liquid Programming for Front-end Services:\nUtilized Liquid Programming for front-end services, enhancing the user interface and providing dynamic and personalized content delivery. Contributed to creating a seamless and user-friendly experience for clients interacting with the billing solution. Impact and Value Delivered My role at Tata Consultancy Services had a significant impact on the development and optimization of a SAAS Cloud-Based Billing Solution, resulting in streamlined workflows, significant cost savings, and improved system performance. The automation of data reporting workflows and the implementation of efficient email services scheduling demonstrated not only technical expertise but also a commitment to delivering tangible business value.\nThroughout this tenure, I honed my skills in API development, SQL queries, and system optimization, showcasing a holistic approach to system engineering and a dedication to driving positive outcomes for both clients and the organization.\n","permalink":"https://live2awesome.github.io/portfoliowebsite/experience/tcs/","summary":"Full-time Role | Tata Consultancy Services (TCS) | Nagpur,India\nOverview As an Assistant System Engineer at Tata Consultancy Services (TCS), I played a pivotal role in developing and enhancing a SAAS Cloud-Based Billing Solution with a multinational client, utilizing Zuora. My responsibilities extended to automating the data reporting workflow for financial data, optimizing services such as SAP, AWS S3 Bucket, and Revpro. Notable achievements include cost-saving initiatives and contributions to API development, SQL queries, and Liquid Programming for front-end services.","title":"Assistant System Engineer"},{"content":"Project: Improving Natural Disaster and Water Resource Management using NLP Overview As a Machine Learning Engineer at Omdena, I contributed to a two-month challenge aimed at enhancing Water Resource Management (WRM) by leveraging space data. The project addressed the impact of natural calamities, such as droughts and floods, on water resources.\nProblem Statement The goal was to ensure sustainable water availability for drinking, sanitation, food production, energy generation, and industry. The challenge involved developing innovative solutions for WRM amidst population growth, urbanization, changing dietary habits, and climate uncertainties.\nProject Outcomes Built a multifaceted NLP tool enabling users to query critical information from the internet. Users could define the topic, area, and time for data gathering, supporting queries like flood evaluations, drought indications, landcover mapping, and urban climate insights. Developed an efficient data storage and evaluation system, providing statistical insights on the gathered information. Achievements Created an NLP tool for improving disaster response and water resource management. Deployed the Streamlit application on Google Cloud Platform Docker. Conducted web scraping using the GooglePyNews package for flood-related events. Designed the deployment architecture aligning with business requirements. Recommendations **Inuri Illeperuma (Product Manager) **Rosana de Oliveira Gomes (Senior Data Scientist) Project Details For more information about the internship project, you can visit Omdena\u0026rsquo;s AI Water Management Project Page.\nThis experience enhanced my machine learning and data engineering skills, emphasizing the practical application of technology for societal and environmental impact.\n","permalink":"https://live2awesome.github.io/portfoliowebsite/experience/omdenaweowater/","summary":"Project: Improving Natural Disaster and Water Resource Management using NLP Overview As a Machine Learning Engineer at Omdena, I contributed to a two-month challenge aimed at enhancing Water Resource Management (WRM) by leveraging space data. The project addressed the impact of natural calamities, such as droughts and floods, on water resources.\nProblem Statement The goal was to ensure sustainable water availability for drinking, sanitation, food production, energy generation, and industry. The challenge involved developing innovative solutions for WRM amidst population growth, urbanization, changing dietary habits, and climate uncertainties.","title":"Machine learning Engineer (Internship)"},{"content":"Overview I had the privilege of interning with The Sparks Foundation, working remotely from Singapore. During this two-month internship, I actively contributed to a significant project in the field of data science.\nProject: Retail Customer Segmentation Usecase The primary focus of my internship at The Sparks Foundation was the implementation of a Retail Customer Segmentation Usecase based on clustering techniques. This involved leveraging data science methodologies to categorize retail customers into distinct segments for targeted business strategies.\nKey Contributions and Achievements Clustering Techniques: Applied various clustering algorithms to effectively segment retail customers based on their behavior, preferences, and purchase history.\nData Analysis: Conducted in-depth data analysis to derive meaningful insights into customer segments, helping the business understand and cater to diverse customer needs.\nVisualization: Created insightful visualizations to communicate findings and trends to stakeholders, making complex data more accessible.\nRecommendations: Developed actionable recommendations for the business based on the identified customer segments, enabling targeted marketing strategies and personalized customer experiences.\nTechnologies Used Programming Languages: Python Data Science Libraries: Pandas, NumPy, Scikit-learn Visualization Tools: Matplotlib, Seaborn Impact The successful implementation of the Retail Customer Segmentation Usecase contributed to enhancing the overall understanding of customer behavior for the business. The insights gained from this project empowered the company to tailor its strategies and services to meet the specific needs of different customer segments, ultimately leading to improved customer satisfaction and business growth.\nThis internship not only provided me with valuable hands-on experience in data science but also allowed me to make a meaningful impact on real-world business challenges.\n","permalink":"https://live2awesome.github.io/portfoliowebsite/experience/sparkfoundation/","summary":"Overview I had the privilege of interning with The Sparks Foundation, working remotely from Singapore. During this two-month internship, I actively contributed to a significant project in the field of data science.\nProject: Retail Customer Segmentation Usecase The primary focus of my internship at The Sparks Foundation was the implementation of a Retail Customer Segmentation Usecase based on clustering techniques. This involved leveraging data science methodologies to categorize retail customers into distinct segments for targeted business strategies.","title":"Data Science Intern"},{"content":"Project: Delivery Route Optimization in LATAM using AI Planning Introduction In collaboration with Omdena and partner Carryt, I contributed to a project focused on optimizing last-mile logistics in congested cities like Bogota, Lima, Mexico City, and Rio de Janeiro. The goal was to improve logistics using artificial intelligence and route planning to address the challenges posed by urban congestion, especially with the rise of e-commerce after the COVID-19 pandemic.\nAbout Carryt Carryt, a Colombian technology company, aims to optimize routes for last-mile logistics. They provide gig economy opportunities for drivers, ensuring a livable wage while offering efficient delivery services. The company operates in Mexico and Brazil, with a significant increase in deliveries from 200K per month in 2021 to a target of 1 million per month in 2022.\nData Processing and Modeling The internship involved working with proprietary geospatial data and exploring alternative modeling approaches. We utilized Operational Research tools (OR-tools) from Google AI for finding near-optimal routes, addressing two types of vehicle routing problems: deliveries picked up from a depot and delivered across the city, and items picked up and then dropped off in various parts of the city.\nData Wrangling The project included processing datasets provided by Carryt to create a map of Bogota for routing. This involved using shapefiles and Open Street Maps (OSM) files to extract critical information such as average speeds for road segments. Python\u0026rsquo;s GeoPandas and NetworkX were employed for efficient data handling and creating a routing map.\nModeling Over ten weeks, the team built algorithms using OR-tools to find optimal routes for vehicles with various constraints. These constraints included vehicle start/stop locations, capacity, time windows for pickups and dropoffs, time spent at each location, and priority considerations. The project addressed challenges akin to the Traveling Salesman Problem (TSP) and the Vehicle Routing Problem (VRP), utilizing state-of-the-art techniques in analytics and machine learning for real-world applications.\nOptimization Techniques The project explored various optimization techniques, including Divide and Conquer using a grid of cells, finding the nearest edge for correct routing, incorporating OSM restriction sets in the A* algorithm, modifying A* to track optimal edge costs, and implementing a Redis cache for performance improvement.\nFuture Work The internship laid the groundwork for future enhancements, including handling infeasible routing problems, exploring additional parameters in OR-tools for better user feedback, providing alternate solutions, building a more efficient caching strategy, and implementing a hierarchical A* path search algorithm.\nRead More on Omdena\u0026rsquo;s AI Water Management Project This experience deepened my understanding of AI planning, route optimization, and real-world applications of machine learning in addressing complex logistics challenges.\n","permalink":"https://live2awesome.github.io/portfoliowebsite/experience/omdenacarryt/","summary":"Project: Delivery Route Optimization in LATAM using AI Planning Introduction In collaboration with Omdena and partner Carryt, I contributed to a project focused on optimizing last-mile logistics in congested cities like Bogota, Lima, Mexico City, and Rio de Janeiro. The goal was to improve logistics using artificial intelligence and route planning to address the challenges posed by urban congestion, especially with the rise of e-commerce after the COVID-19 pandemic.\nAbout Carryt Carryt, a Colombian technology company, aims to optimize routes for last-mile logistics.","title":"Machine Learning Engineer (Internship)"},{"content":"Description Guide: **Mr Arjun Reddy **\nDuring my internship at COSS INDIA , I had the invaluable opportunity to delve deep into Linux systems and expand my expertise in various areas. The internship was a comprehensive exploration of Linux file systems, commands, and server configurations, contributing significantly to my proficiency as a Linux administrator.\nLinux System Administration One of the primary focuses of my internship was mastering Linux file systems and commands. I gained hands-on experience in efficiently navigating, manipulating, and managing files and directories. This foundational knowledge was essential for advanced tasks and troubleshooting, setting a solid base for my role.\nServer Configurations I honed my skills in setting up and configuring FTP servers, NFS servers, and SFTP servers. This included understanding the intricacies of each server type, ensuring secure data transfer, and implementing best practices for server management. The practical exposure to server configurations significantly enhanced my understanding of system architecture.\nFile System Management My internship also included in-depth learning about mounting and unmounting file systems. I gained proficiency in seamlessly attaching and detaching storage devices, optimizing file system performance, and ensuring data integrity during these processes.\nLab Setup and Network Configuration A crucial aspect of my internship was the hands-on experience in setting up Linux servers and configuring networks. This involved applying Red Hat Certified Administrator System (RHCSA) principles, ensuring secure and efficient communication across the infrastructure.\nPython for Data Science To complement my Linux administration skills, I dedicated two days to an intensive Python for Data Science workshop. This allowed me to expand my skill set into the realm of data analysis and manipulation, providing a broader perspective on the integration of Linux systems with data-driven processes.\nCertifications By the end of the internship, I had covered all the content of the Red Hat Certified Administrator System (RHCSA) and half of the Red Hat Certified Engineer (RHCE) curriculum. This not only validated my practical knowledge but also set the stage for pursuing advanced certifications in Linux system administration.\nMy Red Hat internship was a transformative experience, equipping me with a comprehensive skill set in Linux system administration and reinforcing my commitment to excellence in the field. These hands-on experiences have not only broadened my technical expertise but also deepened my passion for creating robust and secure computing environments.\n","permalink":"https://live2awesome.github.io/portfoliowebsite/experience/cossindia/","summary":"Description Guide: **Mr Arjun Reddy **\nDuring my internship at COSS INDIA , I had the invaluable opportunity to delve deep into Linux systems and expand my expertise in various areas. The internship was a comprehensive exploration of Linux file systems, commands, and server configurations, contributing significantly to my proficiency as a Linux administrator.\nLinux System Administration One of the primary focuses of my internship was mastering Linux file systems and commands.","title":"Linux Administrator (Internship)"}]